#-*-coding:utf-8-*-
import time
import os
import shutil
import zipfile
import urllib
from lxml import html
import requests

# Ping if the domain name is responding or not
def pingUrl(hostname):
    response = os.system("ping -n 1 " + hostname + " > NUL 2>&1")

    #and then check the response...
    if response == 0:
        print hostname, 'is up!'
        return True
    else:
        print hostname, 'is down!'
        return False

# Use pingUrl to check and put responsive domain names into pingAliveUrls.txt
def checkValidUrlsByPing():
    if os.path.isfile('pingAliveUrls.txt'):
        os.remove("pingAliveUrls.txt")

    pingAliveUrlFile = open('pingAliveUrls.txt', 'a')
    allValidUrlsFile = open('allValidUrls.txt', 'r+')

    counter = 0
    aliveCounter = 0

    for validUrl in allValidUrlsFile:
        counter += 1
        if pingUrl(validUrl.rstrip()):
            aliveCounter += 1
            pingAliveUrlFile.write(validUrl + "\n")

    pingAliveUrlFile.close()
    allValidUrlsFile.close()

    return counter, aliveCounter

# 1. Download malicious domain names online
# 2. Check if there are urls the same as online urls.
#     If yes, put them into checkedMaliciousUrls.txt
def checkValidUrlByDomainNameList():
    if os.path.isfile('justdomains.zip'):
        os.remove("justdomains.zip")

    if os.path.isfile('checkedMaliciousUrls.txt'):
        os.remove("checkedMaliciousUrls.txt")

    if os.path.isdir('justdomains'):
        shutil.rmtree('justdomains')

    # Download the domain-name-list file from the website
    downloadedFile = urllib.URLopener()
    downloadedFile.retrieve("http://www.malware-domains.com/files/justdomains.zip", "justdomains.zip")

    # Extract the file
    fh = open('justdomains.zip', 'rb')
    z = zipfile.ZipFile(fh)
    for name in z.namelist():
        outpath = "E:\\Dropbox\\PROGRAMS\\Workspce_Python\\p_git_MalwareAnalysis\\justdomains"
        z.extract(name, outpath)
    fh.close()

    # Start to check malicious domain names by the domain-name-list file
    maliciousDomainNameFile = open('justdomains\\justdomains', 'r+')
    validUrlsNoRepFile = open('allValidUrls.txt', 'r+')

    maliciousDomainNameList = []
    for maliciousDomainName in maliciousDomainNameFile:
        if maliciousDomainName.strip():
            maliciousDomainNameList.append(maliciousDomainName.rstrip().replace("http://", '').replace("https://", ''))

    maliciousDomainNameList.sort()

    # Generate validUrlNoRepList in validUrlsNoRepFile:
    validUrlNoRepList = []
    for validUrlNoRep in validUrlsNoRepFile:
        if validUrlNoRep.strip():
            validUrlNoRepList.append(validUrlNoRep.rstrip().replace("http://", '').replace("https://", ''))

    # Delete validUrlsNoRep.txt. If not in the domain name list, then add it back.
    validUrlsNoRepFile.close()
    if os.path.isfile('allValidUrls.txt'):
        os.remove("allValidUrls.txt")
    validUrlsNoRepFile = open('allValidUrls.txt', 'a')

    # Compare validUrlNoRep with maliciousDomainName
    checkedMaliciousUrlsFile = open('checkedMaliciousUrls.txt', 'a')

    for validUrlNoRep in validUrlNoRepList:
        foundSame = False
        for maliciousDomainName in maliciousDomainNameList:
            if validUrlNoRep < maliciousDomainName:
                break
            elif validUrlNoRep == maliciousDomainName:
                # print "YES"
                foundSame = True
                checkedMaliciousUrlsFile.write(validUrlNoRep + '\n')
                break

        if foundSame == False:
            validUrlsNoRepFile.write(validUrlNoRep + '\n')

    checkedMaliciousUrlsFile.close()
    maliciousDomainNameFile.close()
    validUrlsNoRepFile.close()

if __name__ == "__main__":
    # Check urls in two ways
    # 1. Malicious domain names downloaded from the website
    # 2. Ping remained urls
    if os.path.isfile('pingAliveUrls.txt'):
        os.remove("pingAliveUrls.txt")

    if os.path.isfile('checkedMaliciousUrls.txt'):
        os.remove("checkedMaliciousUrls.txt")

    print "\n###+++ Start to use online domain name list to check if existing urls are the same or not +++###"
    checkValidUrlByDomainNameList()
    print "___--- End of using online domain name list to check if existing urls are the same or not ---___\n"

    print "\n###+++ Start to ping remaining urls +++###"
    tStart = time.time()
    urlCounter, aliveCounter = checkValidUrlsByPing()
    tEnd = time.time()
    print "___--- End of pinging ---___\n"

    print "\n+++Ping Execution Summary+++"
    print "Pinged", urlCounter, "urls..."
    print aliveCounter, "ALIVE..."
    print "\nExecution time:", (tEnd - tStart), "seconds..."
    print "The average time for pinging one url is", (tEnd - tStart) / float(urlCounter), "seconds"
    print "---Ping Execution Summary---\n"