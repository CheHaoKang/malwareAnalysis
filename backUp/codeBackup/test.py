#-*-coding:utf-8-*-
import os
import shutil
import zipfile
import urllib
from lxml import html
import requests
import time
import string


__author__ = 'decken'

#import pymdmp

def run():
    print ("hello")

def pingUrl(hostname):
    response = os.system("ping -n 1 " + hostname + " > NUL 2>&1")

    #and then check the response...
    if response == 0:
        print hostname, 'is up!'
        return True
    else:
        print hostname, 'is down!'
        return False

def checkValidUrlsByPing():
    if os.path.isfile('pingAliveUrls.txt'):
        os.remove("pingAliveUrls.txt")

    pingAliveUrlFile = open('pingAliveUrls.txt', 'a')
    validUrlsNoRepFile = open('validUrlsNoRep.txt', 'r+')

    counter = 0
    aliveCounter = 0

    for validUrl in validUrlsNoRepFile:
        counter += 1
        if pingUrl(validUrl.rstrip()):
            aliveCounter += 1
            pingAliveUrlFile.write(validUrl + "\n")
        #print validUrl
        # if validUrl[0:6] == "Valid:":
        #     #print validUrl[7:]
        #     if pingUrl(validUrl[7:]):
        #         pingAliveUrlFile.write(validUrl[7:] + "\n")

    pingAliveUrlFile.close()
    validUrlsNoRepFile.close()

    return counter, aliveCounter

def checkValidUrlByDomainNameList():
    if os.path.isfile('justdomains.zip'):
        os.remove("justdomains.zip")

    if os.path.isfile('checkedMaliciousUrls.txt'):
        os.remove("checkedMaliciousUrls.txt")

    if os.path.isdir('justdomains'):
        shutil.rmtree('justdomains')

    # Download the domain-name-list file from the website
    downloadedFile = urllib.URLopener()
    downloadedFile.retrieve("http://www.malware-domains.com/files/justdomains.zip", "justdomains.zip")

    # Extract the file
    fh = open('justdomains.zip', 'rb')
    z = zipfile.ZipFile(fh)
    for name in z.namelist():
        outpath = "E:\\Dropbox\\PROGRAMS\\Workspce_Python\\p_git_MalwareAnalysis\\justdomains"
        z.extract(name, outpath)
    fh.close()

    # Start to check malicious domain names by the domain-name-list file
    maliciousDomainNameFile = open('justdomains\\justdomains', 'r+')
    validUrlsNoRepFile = open('validUrlsNoRep.txt', 'r+')

    maliciousDomainNameList = []
    for maliciousDomainName in maliciousDomainNameFile:
        # if not maliciousDomainName.strip():
        #     print "WHITE"
        #
        # if maliciousDomainName != "\n" or maliciousDomainName != "":
        if maliciousDomainName.strip():
            maliciousDomainNameList.append(maliciousDomainName.rstrip().replace("http://", '').replace("https://", ''))
            # print "++++++++11111+++++++++++++"
            # print maliciousDomainName.rstrip()
            # print maliciousDomainName.rstrip().replace("http://", '')
            # print maliciousDomainName.rstrip().replace("https://", '')
            # print "-------------------------------"

    maliciousDomainNameList.sort()

    # Generate validUrlNoRepList in validUrlsNoRepFile:
    validUrlNoRepList = []
    for validUrlNoRep in validUrlsNoRepFile:
        # if validUrlNoRep != "\n" or validUrlNoRep != "":
        if validUrlNoRep.strip():
            validUrlNoRepList.append(validUrlNoRep.rstrip().replace("http://", '').replace("https://", ''))
            # print "++++++++++22222+++++++++++"
            # print validUrlNoRep.rstrip()
            # print validUrlNoRep.rstrip().replace("http://", '').replace("https://", '')
            # print validUrlNoRep.rstrip().replace("https://", '').replace("http://", '')
            # print "-------------------------------"
        # if validUrlNoRep[0:6] == "Valid:":
        #     validUrlNoRepList.append(validUrlNoRep[7:].rstrip())
        #     print "++++++++++22222+++++++++++"
        #     print validUrlNoRep[7:].rstrip()
        #     print validUrlNoRep[7:].rstrip().replace("http://", '')
        #     print validUrlNoRep[7:].rstrip().replace("https://", '')
        #     print "-------------------------------"

    # Delete validUrlsNoRep.txt. If not in the domain name list, then add it back.
    validUrlsNoRepFile.close()
    if os.path.isfile('validUrlsNoRep.txt'):
        os.remove("validUrlsNoRep.txt")
    validUrlsNoRepFile = open('validUrlsNoRep.txt', 'a')

    # Compare validUrlNoRep with maliciousDomainName
    checkedMaliciousUrlsFile = open('checkedMaliciousUrls.txt', 'a')

    for validUrlNoRep in validUrlNoRepList:
        foundSame = False
        for maliciousDomainName in maliciousDomainNameList:
            if validUrlNoRep < maliciousDomainName:
                break
            elif validUrlNoRep == maliciousDomainName:
                # print "YES"
                foundSame = True
                checkedMaliciousUrlsFile.write(validUrlNoRep + '\n')
                break

        if foundSame == False:
            validUrlsNoRepFile.write(validUrlNoRep + '\n')

    checkedMaliciousUrlsFile.close()
    maliciousDomainNameFile.close()
    validUrlsNoRepFile.close()

    # print maliciousDomainNameList, "\n\n\n"
    # print validUrlNoRepList

def combineValidUrls():
    if not os.path.isdir('RESULTS'):
        print "No results found..."
        return

    if os.path.isfile('allValidUrls.txt'):
        os.remove("allValidUrls.txt")

    allValidUrlsFile = open('allValidUrls.txt', 'a+')
    allValidUrlsList = []
    for root, dirs, files in os.walk("RESULTS/"):
        for f in files:
            filePath = os.path.join(root, f)
            if string.rfind(filePath, '_validUrlsNoRep.txt') != -1:
                validUrlsNoRepFile = open(filePath, 'r+')

                for line in validUrlsNoRepFile:
                    if line.strip():
                        line = line.rstrip()
                        same = False

                        for url in allValidUrlsList:
                            if line == url:
                                same = True
                                break

                        if same == False:
                            allValidUrlsList.append(line)

                validUrlsNoRepFile.close()

    allValidUrlsList.sort()
    for url in allValidUrlsList:
        allValidUrlsFile.write(url + '\n')

    allValidUrlsFile.close()

if __name__ == "__main__":
    combineValidUrls()

    # if os.path.isfile('pingAliveUrls.txt'):
    #     os.remove("pingAliveUrls.txt")
    #
    # pingAliveUrlFile = open('pingAliveUrls.txt', 'a')
    # validUrlsNoRepFile = open('validUrlsNoRep.txt', 'r+')
    #
    # for validUrl in validUrlsNoRepFile:
    #     #print validUrl
    #     if validUrl[0:6] == "Valid:":
    #         #print validUrl[7:]
    #         if pingUrl(validUrl[7:]):
    #             pingAliveUrlFile.write(validUrl[7:] + "\n")
    #
    #
    # pingAliveUrlFile.close()
    # validUrlsNoRepFile.close()

    # checkValidUrlByDomainNameList()
    # tStart = time.time()
    # urlCounter, aliveCounter = checkValidUrlsByPing()
    # tEnd = time.time()
    #
    # print "\n+++Ping Execution Summary+++"
    # print "Pinged", urlCounter, "urls..."
    # print aliveCounter, "ALIVE..."
    # print "\nExecution time:", (tEnd - tStart), "seconds..."
    # print "The average time for pinging one url is", (tEnd - tStart) / float(urlCounter)
    # print "---Ping Execution Summary---\n"
    #######The below is going to parse the domain name list from the website#######
    #page = requests.get('http://econpy.pythonanywhere.com/ex/001.html')
    #page = requests.get('http://www.malware-domains.com/files/justdomains.zip')
    #print len(page.content)

    # with open("justdomains.zip", "wb") as handle:
    # for data in tqdm(response.iter_content()):
    #     handle.write(data)
    #tree = html.fromstring(page.content)
    #This will create a list of buyers:
    #buyers = tree.xpath('//div[@title="buyer-name"]/text()')
    #This will create a list of prices
    #prices = tree.xpath('//span[@class="item-price"]/text()')
    #output = tree.xpath('//a[@href="justdomains.zip"]/text()')

    #print 'Output: ', output
    #print 'Prices: ', prices

    #checkValidUrlByDomainNameList()

    # list = []
    # urlFile = open('validUrls.txt', 'r+')
    # for line in urlFile:
    #     list.append(line)
    # print list

    # list = ['111', '222', '333', '444', '555']
    # for i in xrange(len(list)):
    #     if i == "222":
    #         list.remove("222")
    #     #    print i
    #     print i
    # print list

    #+++ Local test
    #maliciousDomainNameFile = open('justdomains\\justdomains', 'r+')
    #
    # maliciousDomainNameList = []
    # for maliciousDomainName in maliciousDomainNameFile:
    #     # if not maliciousDomainName.strip():
    #     #     print "WHITE"
    #     #
    #     # if maliciousDomainName != "\n" or maliciousDomainName != "":
    #     if maliciousDomainName.strip():
    #         maliciousDomainNameList.append(maliciousDomainName.rstrip().replace("http://", '').replace("https://", ''))
    #         # print "++++++++11111+++++++++++++"
    #         # print maliciousDomainName.rstrip()
    #         # print maliciousDomainName.rstrip().replace("http://", '')
    #         # print maliciousDomainName.rstrip().replace("https://", '')
    #         # print "-------------------------------"
    #
    # maliciousDomainNameList.sort()
    #
    # for i in maliciousDomainNameList:
    #     print i
    #
    # validUrl = "globalnursesonline.com"
    # for maliciousDomainName in maliciousDomainNameList:
    #     if validUrl < maliciousDomainName:
    #         print validUrl, maliciousDomainName
    #         break
    #     elif validUrl == maliciousDomainName:
    #         print "#$%"
    #
    # maliciousDomainNameFile.close()
    #--- Local test