#encoding=utf-8
from __future__ import unicode_literals
from ctypes import *
import ctypes
import collections
import re
import os
from winappdbg import win32
import string
import win32con
import win32api
import winerror
import sys
import gc
from ctypes import *
from time import time, sleep
from ctypes.wintypes import *
from threading import Thread
from struct import calcsize, pack, unpack, Struct
from win32gui import PyGetString, PySetString
from win32process import GetProcessMemoryInfo
from win32api import GetCurrentProcessId, GetCurrentProcess, CloseHandle
from win32con import MEM_FREE, PROCESS_VM_READ, PROCESS_VM_WRITE, PROCESS_QUERY_INFORMATION
from win32con import PAGE_READWRITE, PAGE_WRITECOPY, PAGE_EXECUTE_READWRITE, PAGE_EXECUTE_WRITECOPY
from win32con import PAGE_EXECUTE_READ, PAGE_READONLY, PROCESS_VM_OPERATION, PROCESS_ALL_ACCESS
import os
import shutil
import zipfile
import urllib
from lxml import html
import requests

fileCounter = 0

def process_list():
    hProcessSnap = win32.CreateToolhelp32Snapshot()
    pe32 = win32.Process32First(hProcessSnap)
    if pe32 == None:
        print >> sys.stderr, "Failed getting first process."
        return
    while True:
        # szExeFile
        #    The name of the executable file for the process
        # https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/
        hProcess = -1
        try:
            hProcess = win32.OpenProcess(win32.PROCESS_QUERY_INFORMATION | win32.PROCESS_VM_READ,
                                         False, pe32.th32ProcessID)
        except:
            print "Get process handle FAILED:", ctypes.WinError()
        # if hProcess == ["Error 5", "Error 87"]:
        #     print "hProcess:", hProcess

        # print pe32.szExeFile
        # if pe32.th32ProcessID != 0 and pe32.th32ProcessID != 4:  # 0 - System Idle Process; 4 - System Process
        #     print "T process_list():", hProcess, pe32.th32ProcessID, pe32.szExeFile
        #     hProcess = win32.OpenProcess(win32.PROCESS_QUERY_INFORMATION | win32.PROCESS_VM_READ,
        #                               False, pe32.th32ProcessID)
        # else:
        #     hProcess = -1
        #     print "F process_list():", hProcess, pe32.th32ProcessID, pe32.szExeFile

        yield hProcess, pe32.th32ProcessID, pe32.szExeFile  # print pe32.szExeFile
        pe32 = win32.Process32Next(hProcessSnap)
        if pe32 == None:
            break

    # CloseHandle(hProcessSnap)

MEMORY_STATES = {0x1000: "MEM_COMMIT", 0x10000: "MEM_FREE", 0x2000: "MEM_RESERVE"}
MEMORY_PROTECTIONS = {0x10: "PAGE_EXECUTE", 0x20: "PAGE_EXECUTE_READ", 0x40: "PAGE_EXECUTE_READWRITE",
                      0x80: "PAGE_EXECUTE_WRITECOPY", 0x01: "PAGE_NOACCESS", 0x02: "PAGE_READONLY",
                      0x04: "PAGE_READWRITE", 0x08: "PAGE_WRITECOPY"}
MEMORY_TYPES = {0x1000000: "MEM_IMAGE", 0x40000: "MEM_MAPPED", 0x20000: "MEM_PRIVATE"}

class PyMEMORY_BASIC_INFORMATION:
    def __init__(self, MBI):
        self.MBI = MBI
        self.set_attributes()

    def set_attributes(self):
        self.BaseAddress = self.MBI.BaseAddress
        self.AllocationBase = self.MBI.AllocationBase
        self.AllocationProtect = MEMORY_PROTECTIONS.get(self.MBI.AllocationProtect, self.MBI.AllocationProtect)
        self.RegionSize = self.MBI.RegionSize
        self.State = MEMORY_STATES.get(self.MBI.State, self.MBI.State)
        # self.Protect = self.MBI.Protect  # uncomment this and comment next line if you want to do a bitwise check on Protect.
        self.Protect = MEMORY_PROTECTIONS.get(self.MBI.Protect, self.MBI.Protect)
        self.Type = MEMORY_TYPES.get(self.MBI.Type, self.MBI.Type)

def VirtualQueryEx(hProcess, lpAddress):
    _VirtualQueryEx = windll.kernel32.VirtualQueryEx
    _VirtualQueryEx.argtypes = [HANDLE, LPVOID, win32.PMEMORY_BASIC_INFORMATION, win32.SIZE_T]
    _VirtualQueryEx.restype  = win32.SIZE_T

    lpBuffer  = win32.MEMORY_BASIC_INFORMATION()
    dwLength  = sizeof(win32.MEMORY_BASIC_INFORMATION)
    success   = _VirtualQueryEx(hProcess, lpAddress, byref(lpBuffer), dwLength)
    if success == 0:
        raise ctypes.WinError()
    return PyMEMORY_BASIC_INFORMATION(lpBuffer)

def scan_page(process_handle, page_address):
    try:
        information = VirtualQueryEx(process_handle, page_address)
    except:
        print "VirtualQueryEx FAILED:", ctypes.WinError()
        return "", False, "", "", ""

    base_address = information.BaseAddress
    access_protection = information.Protect
    region_size = information.RegionSize
    next_region = base_address + region_size

    print "++++++++++++++++++++++++++++"
    print "access_protection:" + str(access_protection), "    base_address:" + str(hex(base_address))
    print "---------------------------------------------"

    if access_protection not in ["PAGE_EXECUTE_READWRITE"]:
        return next_region, False, base_address, region_size, access_protection
    else:
        # Extract all kinds of permissions and return
        return next_region, True, base_address, region_size, access_protection

def scan_process(process_handle):
    """scans a processes pages for the target value."""
    si = win32.SYSTEM_INFO()
    psi = byref(si)
    windll.kernel32.GetSystemInfo(psi)
    # get the first address of the first page to scan so we know where to start scanning
    # A pointer to the lowest memory address accessible to applications and dynamic-link libraries (DLLs)
    base_address = si.lpMinimumApplicationAddress
    # get the last address to scan so we know when to stop scanning.
    # A pointer to the highest memory address accessible to applications and DLLs
    max_address = si.lpMaximumApplicationAddress
    # RWX_addreStorage = list()
    # RWX_sizeStorage = list()
    addrePermSizeDict = {}
    page_address = base_address
    FoundRWX = False

    #print "si.wProcessorArchitecture:" + str(si.wProcessorArchitecture)
    #print "base_address:" + str(base_address) + "   max_address:" + str(max_address)

    while page_address < max_address:
        next_page, found, address, regionSize, permission = scan_page(process_handle, page_address)

        if address != "":
            print "FOUND: ", permission, hex(address)
        else:
            continue

        permissionSize = []
        #RWX_addreSizeDict[hex(RWX_address)] = RWX_regionSize
        permissionSize.append(permission)
        permissionSize.append(regionSize)
        addrePermSizeDict[address] = permissionSize

        if found == True:
            FoundRWX = True
            # print "FOUND: ", permission, hex(address)
            # permissionSize = []
            # #RWX_addreSizeDict[hex(RWX_address)] = RWX_regionSize
            # permissionSize.append(permission)
            # permissionSize.append(regionSize)
            # addrePermSizeDict[address] = permissionSize
            #addrePermission.append(permission)
            # RWX_addreStorage.append(hex(RWX_address))
            # RWX_sizeStorage.append(RWX_regionSize)
        # else:
        #     print ""
            #print "NOT found RWX"
        # found.extend(f)  # list.extend(seq) - Appends the contents of seq to list
        page_address = next_page

        # if len(RWX_storage) >= 60000000:
        #     print "[Warning] Scan ended early because too many addresses were found to hold the target data."
        #     break

    gc.collect()
    return addrePermSizeDict, FoundRWX

def RECheckingDomain(content):
    urlFile = open('validUrls.txt', 'a')
    domainListFile = open('domainList.txt', 'r')
    domainList = []
    for domain in domainListFile:
        domainList.append(domain.rstrip())
    # url = open('url.txt', 'r')
    # content = url.read()
    #print "@@@" + content + "111"

    # ((http://|https://)?:\/\/(?:www\.|(?!www))[^\s\.]+\.[^\s]{2,}|www\.[^\s]+\.[^\s]{2,})
    reString = '(http://|https://)?([a-zA-Z0-9][a-zA-Z0-9\_\-]{1,62}\.)+([a-zA-Z\-]{2,})'# + ([^a-zA-Z0-9/\.]|\n|$)'
    # reString = '/^[a-zA-Z0-9][a-zA-Z0-9-]{1,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}$/'
    reobj = re.compile(reString, re.IGNORECASE | re.MULTILINE)
    m = reobj.finditer(content)

    for i in m:
        print "+++compare+++"
        print i.group()
        print i.group(1), i.group(2), i.group(3)
        print "---compare---"

        TLD = (i.group(3)).upper()
        Valid = False
        for domain in domainList:
            # print "TLD:_" + TLD + "_"
            # print "domain:_" + domain + "_"
            if TLD == domain:
                Valid = True
                break

        if Valid == True:
            urlFile.write("Valid: " + i.group() + "\n")
            print "Valid: " + i.group()

    #urlFile.write('\n')
    #urlFile.write()
    urlFile.close()


# To check if PE exists in this process.
# If yes, dump the whole memory of this process to see whether domain names exist or not.
def dumpMemory(processInfoList):
    for proInfo in processInfoList:
        if not proInfo.processRWXFound:
            continue

        foundPE = False
        PEaddress = []
        print "\n^^^^Check binary files^^^^^"
        print "2. ", proInfo.processName, proInfo.processID
        #if proInfo.processName == "firefox.exe":
        #if proInfo.processID == 1956:

        # Check PE file
        for (address, permSize) in proInfo.processAddresses.items():
            if permSize[0] not in ["PAGE_EXECUTE_READWRITE"]:
                continue

            print "\nAddress:", hex(address), "   Permission:", permSize[0], "   Size:", permSize[1], \
                "   Handle:", proInfo.processHandle
            #print win32.ReadProcessMemory(proInfo.processHandle, address, size)
            print "===Read the first two bytes to check if this is PE file==="
            data = win32.ReadProcessMemory(proInfo.processHandle, address, 2)
            if len(data) != 2:
                raise ctypes.WinError()
            else:
                #if data[0].decode('utf-8') == 'M'.decode('utf-8') and data[1].decode('utf-8') == 'Z'.decode('utf-8'):
                if data[0] == 'M' and data[1] == 'Z':
                    if not foundPE:
                        foundPE = True
                        print "FOUND_PE==>"
                        print data[0]
                        print data[1]
                        print "<=="

                        PEaddress.append(address)

                        urlFile = open('validUrls.txt', 'a')
                        urlFile.write('\n')
                        urlFile.write("\n" + str(proInfo.processName) + " " + str(proInfo.processID) + "\n")
                        urlFile.flush()
                        urlFile.close()
                        break

                    print "FOUND_PE==>"
                    #foundPE = True
                    PEaddress.append(address)
                    print data[0]
                    print data[1]
                    print "<=="

                    data = win32.ReadProcessMemory(proInfo.processHandle, address, permSize[1])
                    print "&&&&&&&&&"
                    #print data
                    print "#########"
                    RECheckingDomain(data)
                # else:
                #     print "NOT_PE==>"
                #     print data
                #     print "<=="
                # print data
        print "___END___\n"

        # if PE file found, then go to other regions
        if PEaddress:
            print "@@@@@@@@@@Found PE file, go through the whole region@@@@@@@@"
            print "3. ", proInfo.processName, proInfo.processID

            for (address, size) in proInfo.processAddresses.items():
                # for addr in PEaddress:
                #     if address == addr:
                #         continue

                #print "Inside other region"
                print "\nAddress:", hex(address), "   Permission:", permSize[0], "   Size:", permSize[1], \
                    "   Handle:", proInfo.processHandle

                # urlFile = open('validUrls.txt', 'a')
                # urlFile.write("\n" + str(proInfo.processName) + " " + str(proInfo.processID) + "\n")
                # urlFile.close()

                try:
                    data = win32.ReadProcessMemory(proInfo.processHandle, address, permSize[1])
                    print "&&&&&&&&&"
                    #print data
                    print "#########"
                    RECheckingDomain(data)
                except:
                    print ctypes.WinError()
            print "-------------------Found PE file, go to other regions-------------------"

        print "____Check binary files____\n"

# def dumpMemoryAPI(processInfoList):
#     dump(SEL_BY_PID, dumpMode, flags, [optionalArgs])

def checkDuplicateValidUrls():
    global fileCounter

    if not os.path.isfile('validUrls.txt'):
        print "No valid urls found..." \
              "Please wait for 20 seconds to run it again!"
        return
	
    urlFile = open('validUrls.txt', 'r+')
    urlList = []
    for line in urlFile.readlines():
        if line == '\n':
            continue

        same = False

        for url in urlList:
            if url == line:
                same = True
                break

        if same == False:
            urlList.append(line)

    if os.path.isfile('validUrlsNoRep.txt'):
        os.remove("validUrlsNoRep.txt")

    urlNoRepFile = open('validUrlsNoRep.txt', 'a')
    for url in urlList:
        #print "___", url[0:6], "___"
        if url[0:6] == "Valid:":
            urlNoRepFile.write(url[7:].rstrip() + '\n')
        #urlNoRepFile.write(url + '\n')

    urlNoRepFile.close()
    urlFile.close()

    if not os.path.isdir('RESULTS'):
        os.mkdir('RESULTS')

    counter = 0
    counterNoRep = 0
    for root, dirs, files in os.walk("RESULTS/"):
        for f in files:
            filePath = os.path.join(root, f)
            index = string.find(filePath, '_validUrls.txt')
            indexNoRep = string.find(filePath, '_validUrlsNoRep.txt')
            if index != -1:
                counter += 1
            if indexNoRep != -1:
                counterNoRep += 1
                #validUrlsNoRepFile = open(filePath, 'r+')

    counter += 1
    shutil.move("validUrls.txt", str(counter) + "_validUrls.txt")
    shutil.move(str(counter) + '_validUrls.txt', 'RESULTS')

    counterNoRep += 1
    shutil.move("validUrlsNoRep.txt", str(counterNoRep) + "_validUrlsNoRep.txt")
    shutil.move(str(counterNoRep) + '_validUrlsNoRep.txt', 'RESULTS')

def pingUrl(hostname):
    response = os.system("ping -n 1 " + hostname + " > NUL 2>&1")

    #and then check the response...
    if response == 0:
        print hostname, 'is up!'
        return True
    else:
        print hostname, 'is down!'
        return False

def checkValidUrlsByPing():
    if os.path.isfile('pingAliveUrls.txt'):
        os.remove("pingAliveUrls.txt")

    pingAliveUrlFile = open('pingAliveUrls.txt', 'a')
    validUrlsNoRepFile = open('validUrlsNoRep.txt', 'r+')

    counter = 0
    aliveCounter = 0

    for validUrl in validUrlsNoRepFile:
        counter += 1
        if pingUrl(validUrl.rstrip()):
            aliveCounter += 1
            pingAliveUrlFile.write(validUrl + "\n")
        #print validUrl
        # if validUrl[0:6] == "Valid:":
        #     #print validUrl[7:]
        #     if pingUrl(validUrl[7:]):
        #         pingAliveUrlFile.write(validUrl[7:] + "\n")

    pingAliveUrlFile.close()
    validUrlsNoRepFile.close()

    return counter, aliveCounter

def checkValidUrlByDomainNameList():
    if os.path.isfile('justdomains.zip'):
        os.remove("justdomains.zip")

    if os.path.isfile('checkedMaliciousUrls.txt'):
        os.remove("checkedMaliciousUrls.txt")

    if os.path.isdir('justdomains'):
        shutil.rmtree('justdomains')

    # Download the domain-name-list file from the website
    downloadedFile = urllib.URLopener()
    downloadedFile.retrieve("http://www.malware-domains.com/files/justdomains.zip", "justdomains.zip")

    # Extract the file
    fh = open('justdomains.zip', 'rb')
    z = zipfile.ZipFile(fh)
    for name in z.namelist():
        outpath = "E:\\Dropbox\\PROGRAMS\\Workspce_Python\\p_git_MalwareAnalysis\\justdomains"
        z.extract(name, outpath)
    fh.close()

    # Start to check malicious domain names by the domain-name-list file
    maliciousDomainNameFile = open('justdomains\\justdomains', 'r+')
    validUrlsNoRepFile = open('validUrlsNoRep.txt', 'r+')

    maliciousDomainNameList = []
    for maliciousDomainName in maliciousDomainNameFile:
        # if not maliciousDomainName.strip():
        #     print "WHITE"
        #
        # if maliciousDomainName != "\n" or maliciousDomainName != "":
        if maliciousDomainName.strip():
            maliciousDomainNameList.append(maliciousDomainName.rstrip().replace("http://", '').replace("https://", ''))
            # print "++++++++11111+++++++++++++"
            # print maliciousDomainName.rstrip()
            # print maliciousDomainName.rstrip().replace("http://", '')
            # print maliciousDomainName.rstrip().replace("https://", '')
            # print "-------------------------------"

    maliciousDomainNameList.sort()

    # Generate validUrlNoRepList in validUrlsNoRepFile:
    validUrlNoRepList = []
    for validUrlNoRep in validUrlsNoRepFile:
        # if validUrlNoRep != "\n" or validUrlNoRep != "":
        if validUrlNoRep.strip():
            validUrlNoRepList.append(validUrlNoRep.rstrip().replace("http://", '').replace("https://", ''))
            # print "++++++++++22222+++++++++++"
            # print validUrlNoRep.rstrip()
            # print validUrlNoRep.rstrip().replace("http://", '').replace("https://", '')
            # print validUrlNoRep.rstrip().replace("https://", '').replace("http://", '')
            # print "-------------------------------"
        # if validUrlNoRep[0:6] == "Valid:":
        #     validUrlNoRepList.append(validUrlNoRep[7:].rstrip())
        #     print "++++++++++22222+++++++++++"
        #     print validUrlNoRep[7:].rstrip()
        #     print validUrlNoRep[7:].rstrip().replace("http://", '')
        #     print validUrlNoRep[7:].rstrip().replace("https://", '')
        #     print "-------------------------------"

    # Delete validUrlsNoRep.txt. If not in the domain name list, then add it back.
    validUrlsNoRepFile.close()
    if os.path.isfile('validUrlsNoRep.txt'):
        os.remove("validUrlsNoRep.txt")
    validUrlsNoRepFile = open('validUrlsNoRep.txt', 'a')

    # Compare validUrlNoRep with maliciousDomainName
    checkedMaliciousUrlsFile = open('checkedMaliciousUrls.txt', 'a')

    for validUrlNoRep in validUrlNoRepList:
        foundSame = False
        for maliciousDomainName in maliciousDomainNameList:
            if validUrlNoRep < maliciousDomainName:
                break
            elif validUrlNoRep == maliciousDomainName:
                # print "YES"
                foundSame = True
                checkedMaliciousUrlsFile.write(validUrlNoRep + '\n')
                break

        if foundSame == False:
            validUrlsNoRepFile.write(validUrlNoRep + '\n')

    checkedMaliciousUrlsFile.close()
    maliciousDomainNameFile.close()
    validUrlsNoRepFile.close()

    # print maliciousDomainNameList, "\n\n\n"
    # print validUrlNoRepList

def combineValidUrls():
    if not os.path.isdir('RESULTS'):
        print "No results found..."
        return

    if os.path.isfile('allValidUrls.txt'):
        os.remove("allValidUrls.txt")

    allValidUrlsFile = open('allValidUrls.txt', 'a+')
    allValidUrlsList = []
    for root, dirs, files in os.walk("RESULTS/"):
        for f in files:
            filePath = os.path.join(root, f)
            if string.rfind(filePath, '_validUrlsNoRep.txt') != -1:
                validUrlsNoRepFile = open(filePath, 'r+')

                for line in validUrlsNoRepFile:
                    if line.strip():
                        line = line.rstrip()
                        same = False

                        for url in allValidUrlsList:
                            if line == url:
                                same = True
                                break

                        if same == False:
                            allValidUrlsList.append(line)

                validUrlsNoRepFile.close()

    allValidUrlsList.sort()
    for url in allValidUrlsList:
        allValidUrlsFile.write(url + '\n')

    allValidUrlsFile.close()

class processInfo:
    processHandle = ""
    processID = ""
    processName = ""
    processAddresses = None
    #processAddressPerm = None
    processRWXFound = False

if __name__ == '__main__':
    if os.path.isfile('validUrls.txt'):
        os.remove("validUrls.txt")

    if os.path.isfile('validUrlsNoRep.txt'):
        os.remove("validUrlsNoRep.txt")

    if os.path.isfile('pingAliveUrls.txt'):
        os.remove("pingAliveUrls.txt")

    if os.path.isfile('checkedMaliciousUrls.txt'):
        os.remove("checkedMaliciousUrls.txt")

    delete = raw_input('Do you want to remove previous results? (y/n) : ')
    if delete == 'y' or delete == 'Y':
        if os.path.isdir('RESULTS'):
            shutil.rmtree('RESULTS')

    ######+++
    # processInfoList = []
    #
    # for p in process_list():
    #     hProcess = p[0]
    #     processID = p[1]
    #     processName = p[2]
    #     print hProcess, processID, processName
    #     if hProcess == -1:
    #         continue
    #
    #     print "\n+++process addresses+++"
    #     # addresses, sizes = scan_process(hProcess)
    #     addrePermSizeDict, FoundRWX = scan_process(hProcess)
    #     # if addresses:
    #     if addrePermSizeDict:
    #         # Sort the dictionary by key
    #         addrePermSizeDict = collections.OrderedDict(sorted(addrePermSizeDict.items()))
    #         proIn = processInfo()
    #         proIn.processHandle = hProcess
    #         proIn.processID = processID
    #         proIn.processName = processName
    #         proIn.processAddresses = {}
    #         proIn.processAddresses = addrePermSizeDict
    #         proIn.processRWXFound = FoundRWX
    #         processInfoList.append(proIn)
    #
    #         print "\n***addresses result***"
    #         #print "1. proIn.processName:", proIn.processName, "   proIn.processID:", proIn.processID
    #         print "1. ", proIn.processName, proIn.processID
    #         for (k, v) in  proIn.processAddresses.items():
    #             print "Address:", hex(k), "   Permission:", v[0], "   Size:", v[1], "   FoundRWX:", proIn.processRWXFound
    #             # print proIn.processAddresses
    #         print "###addresses result###"
    #     # if processInfoList:
    #     #     print processInfoList[0].processID, processInfoList[0].processName, processInfoList[0].processAddresses
    #     print "---process addresses---\n"
    #
    # print "\n\n\n"
    # # To check if PE exists in this process.
    # # If yes, dump the whole memory of this process to see whether domain names exist or not.
    # dumpMemory(processInfoList)
    #dumpMemoryAPI(processInfoList)
    ######---

    checkDuplicateValidUrls()
    #combineValidUrls()

    # Check urls in two ways
    # 1. Malicious domain names downloaded from the website
    # 2. Ping remained urls
    # checkValidUrlByDomainNameList()
    # tStart = time.time()
    # urlCounter, aliveCounter = checkValidUrlsByPing()
    # tEnd = time.time()
    #
    # print "\n+++Ping Execution Summary+++"
    # print "Pinged", urlCounter, "urls..."
    # print aliveCounter, "ALIVE..."
    # print "\nExecution time:", (tEnd - tStart), "seconds..."
    # print "The average time for pinging one url is", (tEnd - tStart) / float(urlCounter)
    # print "---Ping Execution Summary---\n"